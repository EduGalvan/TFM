{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+u85qrJmTTFK89kpn3skB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWscPR8QXuLa","executionInfo":{"status":"ok","timestamp":1689019084174,"user_tz":-120,"elapsed":16376,"user":{"displayName":"edwar6362","userId":"09730295915126954115"}},"outputId":"3f2e3996-51a1-4864-bfc4-da0357824197"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Frases en el texto:\n"," (100, 22)\n","\n","Etiquetas:\n"," (100,)\n","\n","Longitud maxima de cada frase:\n"," 22\n","\n","Longitud maxima de vocabulario:\n"," 10000\n","\n","Label mapping:\n"," [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n","Epoch 1/5\n","3/3 [==============================] - 10s 156ms/step - loss: 1.5164 - accuracy: 0.4857\n","Epoch 2/5\n","3/3 [==============================] - 0s 145ms/step - loss: 1.4743 - accuracy: 0.7286\n","Epoch 3/5\n","3/3 [==============================] - 0s 151ms/step - loss: 1.4027 - accuracy: 0.7429\n","Epoch 4/5\n","3/3 [==============================] - 0s 142ms/step - loss: 1.3063 - accuracy: 0.7429\n","Epoch 5/5\n","3/3 [==============================] - 0s 136ms/step - loss: 1.1468 - accuracy: 0.7286\n","1/1 [==============================] - 1s 962ms/step - loss: 1.1987 - accuracy: 0.6333\n","Test Loss: 1.1987252235412598\n","Test Accuracy: 0.6333333253860474\n"]}],"source":["\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('/content/R6.csv')\n","\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","stop_words = stopwords.words('spanish')\n","\n","df.columns = ['ID', 'R6', \"Conocimiento\"]\n","\n","texts = df['R6'].copy()\n","texts = [text.lower() for text in texts ]\n","texts = [text.split() for text in texts ]\n","texts = [[word.strip() for word in text] for text in texts]\n","texts = [[word for word in text if word not in stop_words ] for text in texts]\n","\n","df[\"words\"] = texts\n","\n","\n","\n","\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","def preprocess_input(df, max_seq_len=None):\n","  texts = df['R6'].copy()\n","  labels = df['Conocimiento'].copy()\n","\n","  stop_words = stopwords.words('spanish')\n","\n","  texts = [text.lower() for text in texts ]\n","  texts = [text.split() for text in texts ]\n","  texts = [[word.strip() for word in text] for text in texts]\n","  texts = [[word for word in text if word not in stop_words ] for text in texts]\n","\n","  vocab_length = 10000\n","\n","  tokenizer = Tokenizer(num_words=vocab_length)\n","  tokenizer.fit_on_texts(texts)\n","  texts = tokenizer.texts_to_sequences(texts)\n","\n","  if max_seq_len is None:\n","    max_seq_len = np.max([len(text) for text in texts])\n","\n","  texts = pad_sequences(texts, maxlen=max_seq_len, padding='post')\n","\n","  labels = np.array(labels)\n","\n","  return texts, labels, max_seq_len, vocab_length, labels\n","\n","texts, labels, max_seq_len, vocab_lenght, label_mapping = preprocess_input(df, 22)\n","print(\"Frases en el texto:\\n\", texts.shape)\n","print(\"\\nEtiquetas:\\n\", labels.shape)\n","print(\"\\nLongitud maxima de cada frase:\\n\", max_seq_len)\n","print(\"\\nLongitud maxima de vocabulario:\\n\", vocab_lenght)\n","print(\"\\nLabel mapping:\\n\", label_mapping)\n","\n","import tensorflow as tf\n","import numpy as np\n","from keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n","from tensorflow.keras import regularizers\n","\n","encoded_labels = np.zeros((len(labels), 3))  # Crear un array de ceros de forma (n, 3)\n","\n","# Codificar los valores manualmente\n","for i, label in enumerate(labels):\n","    encoded_labels[i, label-1] = 1\n","\n","labels = encoded_labels\n","texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, train_size=0.7, random_state=220)\n","model = Sequential()\n","model.add(Embedding(input_dim=vocab_lenght, output_dim= 512, input_length=max_seq_len))\n","model.add(LSTM(units=128, return_sequences=True))\n","model.add(Dropout(0.2))  # Regularizaci√≥n mediante Dropout\n","model.add(LSTM(units=64))\n","model.add(Dense(units=32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n","model.add(Dense(units=3, activation='softmax'))\n","\n","# Compilar el modelo\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Entrenar el modelo\n","model.fit(texts_train, labels_train, epochs=5, batch_size=32)\n","\n","# Evaluar el modelo en el conjunto de prueba\n","loss, accuracy = model.evaluate(texts_test, labels_test)\n","print('Test Loss:', loss)\n","print('Test Accuracy:', accuracy)"]}]}