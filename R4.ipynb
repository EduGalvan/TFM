{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYbuNMj4UnizZ1hbYlJwpE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7BM6KmMeUtyU","executionInfo":{"status":"ok","timestamp":1689019218752,"user_tz":-120,"elapsed":26315,"user":{"displayName":"edwar6362","userId":"09730295915126954115"}},"outputId":"d8431da3-2c5a-4f2f-abd2-76b7309162a7"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Frases en el texto:\n"," (100, 22)\n","\n","Etiquetas:\n"," (100,)\n","\n","Longitud maxima de cada frase:\n"," 22\n","\n","Longitud maxima de vocabulario:\n"," 10000\n","\n","Label mapping:\n"," [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n","Epoch 1/4\n","3/3 [==============================] - 8s 146ms/step - loss: 1.5076 - accuracy: 0.3857\n","Epoch 2/4\n","3/3 [==============================] - 0s 139ms/step - loss: 1.3748 - accuracy: 0.7714\n","Epoch 3/4\n","3/3 [==============================] - 1s 223ms/step - loss: 1.1917 - accuracy: 0.8143\n","Epoch 4/4\n","3/3 [==============================] - 1s 264ms/step - loss: 1.0211 - accuracy: 0.9143\n","1/1 [==============================] - 3s 3s/step - loss: 0.9189 - accuracy: 0.8333\n","Test Loss: 0.9188695549964905\n","Test Accuracy: 0.8333333134651184\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 22, 512)           5120000   \n","                                                                 \n"," lstm (LSTM)                 (None, 22, 128)           328192    \n","                                                                 \n"," dropout (Dropout)           (None, 22, 128)           0         \n","                                                                 \n"," lstm_1 (LSTM)               (None, 64)                49408     \n","                                                                 \n"," dense (Dense)               (None, 32)                2080      \n","                                                                 \n"," dense_1 (Dense)             (None, 3)                 99        \n","                                                                 \n","=================================================================\n","Total params: 5,499,779\n","Trainable params: 5,499,779\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('/content/R4.csv')\n","\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","stop_words = stopwords.words('spanish')\n","\n","df.columns = ['ID', 'R4', \"Conocimiento\"]\n","\n","texts = df['R4'].copy()\n","texts = [text.lower() for text in texts ]\n","texts = [text.split() for text in texts ]\n","texts = [[word.strip() for word in text] for text in texts]\n","texts = [[word for word in text if word not in stop_words ] for text in texts]\n","\n","df[\"words\"] = texts\n","\n","\n","\n","\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","def preprocess_input(df, max_seq_len=None):\n","  texts = df['R4'].copy()\n","  labels = df['Conocimiento'].copy()\n","\n","  stop_words = stopwords.words('spanish')\n","\n","  texts = [text.lower() for text in texts ]\n","  texts = [text.split() for text in texts ]\n","  texts = [[word.strip() for word in text] for text in texts]\n","  texts = [[word for word in text if word not in stop_words ] for text in texts]\n","\n","  vocab_length = 10000\n","\n","  tokenizer = Tokenizer(num_words=vocab_length)\n","  tokenizer.fit_on_texts(texts)\n","  texts = tokenizer.texts_to_sequences(texts)\n","\n","  if max_seq_len is None:\n","    max_seq_len = np.max([len(text) for text in texts])\n","\n","  texts = pad_sequences(texts, maxlen=max_seq_len, padding='post')\n","\n","  labels = np.array(labels)\n","\n","  return texts, labels, max_seq_len, vocab_length, labels\n","\n","texts, labels, max_seq_len, vocab_lenght, label_mapping = preprocess_input(df, 22)\n","print(\"Frases en el texto:\\n\", texts.shape)\n","print(\"\\nEtiquetas:\\n\", labels.shape)\n","print(\"\\nLongitud maxima de cada frase:\\n\", max_seq_len)\n","print(\"\\nLongitud maxima de vocabulario:\\n\", vocab_lenght)\n","print(\"\\nLabel mapping:\\n\", label_mapping)\n","\n","import tensorflow as tf\n","from tensorflow.keras import regularizers\n","import numpy as np\n","from keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n","\n","encoded_labels = np.zeros((len(labels), 3))  # Crear un array de ceros de forma (n, 3)\n","\n","# Codificar los valores manualmente\n","for i, label in enumerate(labels):\n","    encoded_labels[i, label-1] = 1\n","\n","labels = encoded_labels\n","texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, train_size=0.7, random_state=220)\n","model = Sequential()\n","model.add(Embedding(input_dim=vocab_lenght, output_dim= 512, input_length=max_seq_len))\n","model.add(LSTM(units=128, return_sequences=True))\n","model.add(Dropout(0.2))  # Regularizaci√≥n mediante Dropout\n","model.add(LSTM(units=64))\n","model.add(Dense(units=32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n","model.add(Dense(units=3, activation='softmax'))\n","\n","# Compilar el modelo\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Entrenar el modelo\n","model.fit(texts_train, labels_train, epochs=4, batch_size=32)\n","\n","# Evaluar el modelo en el conjunto de prueba\n","loss, accuracy = model.evaluate(texts_test, labels_test)\n","print('Test Loss:', loss)\n","print('Test Accuracy:', accuracy)\n","\n","model.summary()"]}]}