{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgHJYWT5wLysjtiKef37kH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZt4bMwsOL0t","executionInfo":{"status":"ok","timestamp":1689016899679,"user_tz":-120,"elapsed":239,"user":{"displayName":"edwar6362","userId":"09730295915126954115"}},"outputId":"14c50007-9aa6-4910-e2a7-54dba318ba7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Frases en el texto:\n"," (100, 22)\n","\n","Etiquetas:\n"," (100,)\n","\n","Longitud maxima de cada frase:\n"," 22\n","\n","Longitud maxima de vocabulario:\n"," 10000\n","\n","Label mapping:\n"," [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('/content/R2.csv')\n","\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","stop_words = stopwords.words('spanish')\n","\n","df.columns = ['ID', 'R2', \"Conocimiento\"]\n","\n","texts = df['R2'].copy()\n","texts = [text.lower() for text in texts ]\n","texts = [text.split() for text in texts ]\n","texts = [[word.strip() for word in text] for text in texts]\n","texts = [[word for word in text if word not in stop_words ] for text in texts]\n","\n","df[\"words\"] = texts\n","\n","\n","\n","\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","def preprocess_input(df, max_seq_len=None):\n","  texts = df['R2'].copy()\n","  labels = df['Conocimiento'].copy()\n","\n","  stop_words = stopwords.words('spanish')\n","\n","  texts = [text.lower() for text in texts ]\n","  texts = [text.split() for text in texts ]\n","  texts = [[word.strip() for word in text] for text in texts]\n","  texts = [[word for word in text if word not in stop_words ] for text in texts]\n","\n","  vocab_length = 10000\n","\n","  tokenizer = Tokenizer(num_words=vocab_length)\n","  tokenizer.fit_on_texts(texts)\n","  texts = tokenizer.texts_to_sequences(texts)\n","\n","  if max_seq_len is None:\n","    max_seq_len = np.max([len(text) for text in texts])\n","\n","  texts = pad_sequences(texts, maxlen=max_seq_len, padding='post')\n","\n","  labels = np.array(labels)\n","\n","  return texts, labels, max_seq_len, vocab_length, labels\n","\n","texts, labels, max_seq_len, vocab_lenght, label_mapping = preprocess_input(df, 22)\n","print(\"Frases en el texto:\\n\", texts.shape)\n","print(\"\\nEtiquetas:\\n\", labels.shape)\n","print(\"\\nLongitud maxima de cada frase:\\n\", max_seq_len)\n","print(\"\\nLongitud maxima de vocabulario:\\n\", vocab_lenght)\n","print(\"\\nLabel mapping:\\n\", label_mapping)"]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout"],"metadata":{"id":"YDVLvip_PodI","executionInfo":{"status":"ok","timestamp":1689016766108,"user_tz":-120,"elapsed":9,"user":{"displayName":"edwar6362","userId":"09730295915126954115"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["encoded_labels = np.zeros((len(labels), 3))  # Crear un array de ceros de forma (n, 3)\n","\n","# Codificar los valores manualmente\n","for i, label in enumerate(labels):\n","    encoded_labels[i, label-1] = 1\n","\n","labels = encoded_labels\n","texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, train_size=0.7, random_state=220)\n","model = Sequential()\n","model.add(Embedding(input_dim=vocab_lenght, output_dim= 512, input_length=max_seq_len))\n","model.add(LSTM(units=128, return_sequences=True))\n","model.add(Dropout(0.2))  # Regularizaci√≥n mediante Dropout\n","model.add(LSTM(units=64))\n","model.add(Dense(units=32, activation='relu'))\n","model.add(Dense(units=3, activation='softmax'))\n","\n","# Compilar el modelo\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Entrenar el modelo\n","model.fit(texts_train, labels_train, epochs=8, batch_size=32)\n","\n","# Evaluar el modelo en el conjunto de prueba\n","loss, accuracy = model.evaluate(texts_test, labels_test)\n","print('Test Loss:', loss)\n","print('Test Accuracy:', accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DFCoCkgTOr3G","executionInfo":{"status":"ok","timestamp":1689016913884,"user_tz":-120,"elapsed":11309,"user":{"displayName":"edwar6362","userId":"09730295915126954115"}},"outputId":"3cd8f174-e615-41db-d74e-4d320c89a698"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/8\n","3/3 [==============================] - 5s 132ms/step - loss: 1.0975 - accuracy: 0.3714\n","Epoch 2/8\n","3/3 [==============================] - 0s 138ms/step - loss: 1.0385 - accuracy: 0.6571\n","Epoch 3/8\n","3/3 [==============================] - 0s 120ms/step - loss: 0.9573 - accuracy: 0.6714\n","Epoch 4/8\n","3/3 [==============================] - 0s 120ms/step - loss: 0.8314 - accuracy: 0.6714\n","Epoch 5/8\n","3/3 [==============================] - 0s 146ms/step - loss: 0.6282 - accuracy: 0.7286\n","Epoch 6/8\n","3/3 [==============================] - 1s 203ms/step - loss: 0.3856 - accuracy: 0.7714\n","Epoch 7/8\n","3/3 [==============================] - 1s 175ms/step - loss: 0.2752 - accuracy: 0.7714\n","Epoch 8/8\n","3/3 [==============================] - 1s 216ms/step - loss: 0.2218 - accuracy: 0.7714\n","1/1 [==============================] - 1s 849ms/step - loss: 0.4144 - accuracy: 0.9000\n","Test Loss: 0.41442352533340454\n","Test Accuracy: 0.8999999761581421\n"]}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qt7-B4afQWUK","executionInfo":{"status":"ok","timestamp":1689016960242,"user_tz":-120,"elapsed":367,"user":{"displayName":"edwar6362","userId":"09730295915126954115"}},"outputId":"56c5a921-ce4f-4b33-e24d-64fd08f1d763"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 22, 512)           5120000   \n","                                                                 \n"," lstm_4 (LSTM)               (None, 22, 128)           328192    \n","                                                                 \n"," dropout_2 (Dropout)         (None, 22, 128)           0         \n","                                                                 \n"," lstm_5 (LSTM)               (None, 64)                49408     \n","                                                                 \n"," dense_4 (Dense)             (None, 32)                2080      \n","                                                                 \n"," dense_5 (Dense)             (None, 3)                 99        \n","                                                                 \n","=================================================================\n","Total params: 5,499,779\n","Trainable params: 5,499,779\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]}]}